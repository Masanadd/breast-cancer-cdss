import os
import json
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer
from pinecone import Pinecone
import cohere



# Cargar variables de entorno
load_dotenv()

# 游늷 Variables de entorno
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_ENVIRONMENT = os.getenv("PINECONE_ENVIRONMENT")
INDEX_NAME = os.getenv("INDEX_NAME")
COHERE_API_KEY = os.getenv("COHERE_API_KEY")

# 游댌 Modelo de embeddings
EMBEDDING_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"

# Inicializar globalmente para evitar recargas
embedding_model = None
pinecone_index = None
cohere_client = cohere.Client(COHERE_API_KEY)

def load_embedding_model():
    global embedding_model
    if embedding_model is None:
        print(f"游댌 Cargando modelo: {EMBEDDING_MODEL_NAME}")
        embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)
    return embedding_model

def get_pinecone_index():
    global pinecone_index
    if pinecone_index is None:
        print("游댋 Conectando a Pinecone...")
        pc = Pinecone(api_key=PINECONE_API_KEY)
        pinecone_index = pc.Index(INDEX_NAME)
    return pinecone_index

def search_pinecone(query, top_k=5):
    index = get_pinecone_index()
    embedding_model = load_embedding_model()

    print(f"游댍 Buscando en Pinecone: '{query}'")
    query_embedding = embedding_model.encode(query).tolist()

    response = index.query(vector=query_embedding, top_k=top_k, include_metadata=True)

    return [
        {
            "score": match["score"],
            "title": match["metadata"]["title"],
            "text": match["metadata"]["text"]
        }
        for match in response["matches"]
    ]

def generate_answer(query, retrieved_docs):
    # Combina claramente los textos relevantes para el contexto
    context = "\n\n".join([
        f"T칤tulo: {doc['title']}\nContenido: {doc['text']}"
        for doc in retrieved_docs
    ])
    # Clasifica intenci칩n de la consulta
    query_lower = query.lower()
    if "tratamiento" in query_lower or "tratar" in query_lower:
        secciones = "- Responde solo con las opciones de tratamiento m치s relevantes.\n- Proporciona un m칤nimo de contexto solo si es necesario."
    elif "diagn칩stico" in query_lower:
        secciones = "- Lim칤tate a los procedimientos y criterios de diagn칩stico seg칰n el contexto.\n- No incluyas opciones de tratamiento ni recomendaciones."
    elif "recomendaciones" in query_lower:
        secciones = "- Proporciona 칰nicamente recomendaciones cl칤nicas basadas en gu칤as.\n- No repitas diagn칩stico ni tratamientos si no se piden."
    else:
        secciones = "- Organiza la respuesta en: Diagn칩stico, Opciones de Tratamiento y Recomendaciones."


    # Prompt claro, sin referencias num칠ricas
    prompt = f"""
    Eres un onc칩logo senior especializado en c치ncer de mama.
    Usa 칰nicamente la informaci칩n proporcionada en el contexto siguiente para responder.

    CONTEXTO DISPONIBLE:
    {context}

    INSTRUCCIONES:
    - Organiza la respuesta en las siguientes secciones:
      1. Diagn칩stico
      2. Opciones de Tratamiento
      3. Recomendaciones
    - Proporciona una respuesta clara y precisa, priorizando tratamientos basados en biomarcadores (HER2+, HR+, etc.) y clasificaci칩n TNM cuando sea relevante.
    - Si alguna informaci칩n no est치 disponible expl칤citamente en el contexto proporcionado, indica claramente que no dispones de dicha informaci칩n.
    - No utilices referencias como [1], [2] o similares.

    PREGUNTA:
    {query}

    RESPUESTA:
    """

    response = cohere_client.generate(
        model="command-r-plus",
        prompt=prompt,
        max_tokens=800,
        temperature=0.1,
        frequency_penalty=0.5,
        presence_penalty=0.5,
        truncate="END"
    )

    import re
    raw_text = response.generations[0].text.strip()
    cleaned_answer = re.sub(r'\[\d+\]', '', raw_text)
    return cleaned_answer
 

def main():
    load_embedding_model()
    get_pinecone_index()

    while True:
        query = input("\n游닇 Pregunta (o 'exit' para salir): ")
        if query.lower() == "exit":
            print("游녦 Saliendo...")
            break
        
        results = search_pinecone(query)

        print("\n游늷 **Resultados m치s relevantes:**")
        for idx, res in enumerate(results, 1):
            print(f"\n游댳 **{res['title']}** (Score: {res['score']:.4f})")
            print(f"{res['text'][:200]}..." if len(res['text']) > 200 else res['text'])

        answer = generate_answer(query, results)

        print("\n游 **Respuesta generada:**")
        print(answer)

if __name__ == "__main__":
    main()
